---
title: "R Notebook"
output: html_notebook
---

```{r}
library(MASS)
dataset.obj <- as.data.frame(Boston)
summary(dataset.obj)
## change the variable chas to factor, since it's a dummy variable
dataset.obj$chas <- as.factor(dataset.obj$chas)
## verify
summary(dataset.obj$chas)
```

```{r}
## The overall k-fold-based CV approach
samplesize <- nrow(dataset.obj)
samplesize
```
```{r}
numfolds <- 11 # we're setting k = 11

quotient <- samplesize %/% numfolds # the %/% operator returns the quotient of a division
remainder <- samplesize %% numfolds # the %% operator returns the remainder of a division

vct.sizes <- rep(quotient, numfolds) # create a vector representing the initial subsets with size = quotient
if(remainder > 0){
    for(i in 1:remainder){
        vct.sizes[i] <- vct.sizes[i] + 1 # for the "remainder" number of subsets, add one to their size
    }
}

print(paste("K:", 11, "n:", samplesize))
print(vct.sizes)
```

```{r}
startval <- 1
endval <- nrow(dataset.obj)*(9/10)

model <- glm(crim ~ indus + dis + medv,
             data = dataset.obj[-(startval:endval), ])
pred.vals <- predict(model, newdata=dataset.obj[startval:endval, ],
                     type="response")
rmse <- sqrt(mean((dataset.obj$crim[startval:endval] - pred.vals)^2))
```

```{r}
set.seed(112)
```


```{r}
dataset.obj <- dataset.obj[sample(nrow(dataset.obj)), ]
```


```{r}
## create the vector to hold RMSE values
vct.rmses <- numeric(numfolds)

startval <- 1
for(kth in (1:numfolds)){
    endval <- vct.sizes[kth] + startval - 1

    model <- glm(crim ~ indus + dis + medv,
                 data = dataset.obj[-(startval:endval), ])
    pred.vals <- predict(model,
                         newdata = dataset.obj[startval:endval, ],
                         type="response")
    ## compute the RMSE
    rmse <- sqrt(mean((dataset.obj$crim[startval:endval] - pred.vals)^2))
    ## store the current fold's RMSE
    vct.rmses[kth] <- rmse
    ## modify the start value to correspond to the next fold
    startval <- endval + 1
}

## Compute the overall RMSE
overall.rmse <- mean(vct.rmses)
print(paste("For the model crim ~ indus + dis + medv, the overall 11-fold CV RMSE is:",
            round(overall.rmse, 6)))
```

```{r}
set.seed(112)
dataset.obj <- dataset.obj[sample(nrow(dataset.obj)), ]

## create the vector to hold RMSE values
vct.rmses <- numeric(numfolds)

startval <- 1
for(kth in (1:numfolds)){
    endval <- vct.sizes[kth] + startval - 1

    model <- glm(crim ~ indus + dis + medv + age+ rm,
                 data = dataset.obj[-(startval:endval), ])
    pred.vals <- predict(model,
                         newdata = dataset.obj[startval:endval, ],
                         type="response")
    ## compute the RMSE
    rmse <- sqrt(mean((dataset.obj$crim[startval:endval] - pred.vals)^2))
    ## store the current fold's RMSE
    vct.rmses[kth] <- rmse
    ## modify the start value to correspond to the next fold
    startval <- endval + 1
}
```


```{r}
## Compute the overall RMSE
overall.rmse <- mean(vct.rmses)
print(paste("For the model crim ~ indus + dis + medv + age+ rm, the overall 11-fold CV RMSE is:",
            round(overall.rmse, 6)))
```
there is a slight reduction in the models 11 fold cv error, between the models taken the second model rmse looks less compared to the rmse of the former model which shows that the latter model has greater predictor accuracy.

```{r}
getCoeffs <- function(dataset, indices, formula){
    d <- dataset[indices, ]
    fit <- glm(formula, data=d)
    return(coef(fit))
}
library(boot)
set.seed(122)
results <- boot(data=Boston,
                statistic=getCoeffs,
                R=1000,
                formula = crim ~ indus + dis + medv)


print("Coefficients of the model")
print(coef(model))
print("Estimated coefficient values from bootstrap")
print(results)
prednames <- c("indus", "dis", "medv")
# Since we have four coefficients, we will iterate over the computations of CIs, one at a time
for(i in 2:4){
    print(paste("The confidence interval for", prednames[i-1], "is:"))
    print(boot.ci(results, conf=0.95, index=i, type="basic"))
}
```
###########################################################################
```{r}
## Using k-fold CV for building and estimating the performance of a glm model
library(MASS)
library(class)
data(Boston)
housingdataset <- as.data.frame(Boston) # attach a label to the dataframe object

Value <- ifelse(housingdataset$medv > median(housingdataset$medv),
                "High", "Low")
Value <- as.factor(Value)
summary(Value)

housingdataset <- data.frame(housingdataset, Value)
housingdataset$chas <- as.factor(housingdataset$chas)
summary(housingdataset)

for (iternum in (1:3)){
  print(paste("Shuffle number:", iternum))
  housingdataset <- housingdataset[sample(nrow(housingdataset)), ]
}

sz <- nrow(housingdataset) * 0.90
training_testing <- housingdataset[1:sz,]
holdout <- housingdataset[(sz):nrow(housingdataset),]


## k-fold CV

samplesize <- nrow(training_testing)
numfolds <- 7

quotient <- samplesize %/% numfolds
remainder <- samplesize %% numfolds 

vct.sizes <- rep(quotient, numfolds) 
if(remainder > 0){
  for(i in 1:remainder){
    vct.sizes[i] <- vct.sizes[i] + 1 
  }
}

print(paste("K:", numfolds, "n:", samplesize))
print(vct.sizes)

set.seed(100)
training_testing <- training_testing[sample(nrow(training_testing)), ]


accuracies <- numeric(numfolds)
startval <- 1
for(kth in (1:numfolds)){
  endval <- vct.sizes[kth] + startval - 1
  
  glm_model <- glm(Value ~ lstat+rm+ptratio,
                   family=binomial, 
                   data = training_testing[-(startval:endval), ])
  glm_model
  glm.pred.vals <- predict(glm_model,
                           newdata = training_testing[startval:endval, ],
                           type="response")
  glm.pred.vals
  glm.predictions <- rep("High", length(glm.pred.vals))
  glm.predictions [glm.pred.vals > 0.50] <- "Low" 
  glm.predictions
  tbl.results <- table(training_testing[startval:endval, 15], glm.predictions)
  
  ## print(tbl.results)
  
  accuracy <- (round((tbl.results[1,1] + tbl.results[2,2])/sum(tbl.results), 4)*100)
  
  accuracies[kth] <- accuracy
  startval <- endval + 1
}

overall.glm.accuracy <- mean(accuracies)
print(paste("For the best glm model, the overall k-fold CV accuracy is:",
            round(overall.glm.accuracy, 5)))
```

```{r}
#for lda
library(MASS)
library(class)
data(Boston)
housingdataset <- as.data.frame(Boston) # attach a label to the dataframe object

Value <- ifelse(housingdataset$medv > median(housingdataset$medv),
                "High", "Low")
Value <- as.factor(Value)
summary(Value)

housingdataset <- data.frame(housingdataset, Value)
housingdataset$chas <- as.factor(housingdataset$chas)
summary(housingdataset)

for (iternum in (1:3)){
  print(paste("Shuffle number:", iternum))
  housingdataset <- housingdataset[sample(nrow(housingdataset)), ]
}

sz <- nrow(housingdataset) * 0.90
training_testing <- housingdataset[1:sz,]
holdout <- housingdataset[(sz):nrow(housingdataset),]


## k-fold CV

samplesize <- nrow(training_testing)
numfolds <- 7

quotient <- samplesize %/% numfolds
remainder <- samplesize %% numfolds 

vct.sizes <- rep(quotient, numfolds) 
if(remainder > 0){
  for(i in 1:remainder){
    vct.sizes[i] <- vct.sizes[i] + 1 
  }
}

print(paste("K:", numfolds, "n:", samplesize))
print(vct.sizes)

set.seed(100)
training_testing <- training_testing[sample(nrow(training_testing)), ]


## Using k-fold CV for building and estimating the performance of a lda model
accuracies <- numeric(numfolds)
startval <- 1
for(kth in (1:numfolds)){
  endval <- vct.sizes[kth] + startval - 1
  
  LDA_model <- lda(Value ~ lstat+rm+ptratio,
                   family=binomial, 
                   data = training_testing[-(startval:endval),])

  lda.pred.vals <- predict(LDA_model,
                           newdata = training_testing[startval:endval,],
                           type="response")
  
  
  tbl.results <- table(training_testing[startval:endval,]$Value,
                       lda.pred.vals$class)
  
  
  print(tbl.results)
  
  accuracy <- (round((tbl.results[1,1] + tbl.results[2,2])/sum(
    
    1,tbl.results), 4)*100)
  
  accuracies[kth] <- accuracy
  startval <- endval + 1
}

overall.lda.accuracy <- mean(accuracies)
print(paste("For the best lda model, the overall k-fold CV accuracy is:",
            round(overall.lda.accuracy,5)))

```

```{r}
#for qda
library(MASS)
library(class)
data(Boston)
housingdataset <- as.data.frame(Boston) # attach a label to the dataframe object

Value <- ifelse(housingdataset$medv > median(housingdataset$medv),
                "High", "Low")
Value <- as.factor(Value)
summary(Value)

housingdataset <- data.frame(housingdataset, Value)
housingdataset$chas <- as.factor(housingdataset$chas)
summary(housingdataset)

for (iternum in (1:3)){
  print(paste("Shuffle number:", iternum))
  housingdataset <- housingdataset[sample(nrow(housingdataset)), ]
}

sz <- nrow(housingdataset) * 0.90
training_testing <- housingdataset[1:sz,]
holdout <- housingdataset[(sz):nrow(housingdataset),]


## k-fold CV

samplesize <- nrow(training_testing)
numfolds <- 7

quotient <- samplesize %/% numfolds
remainder <- samplesize %% numfolds 

vct.sizes <- rep(quotient, numfolds) 
if(remainder > 0){
  for(i in 1:remainder){
    vct.sizes[i] <- vct.sizes[i] + 1 
  }
}

print(paste("K:", numfolds, "n:", samplesize))
print(vct.sizes)

set.seed(100)
training_testing <- training_testing[sample(nrow(training_testing)), ]
#######################################################################
## Using k-fold CV for building and estimating the performance of a qda model
accuracies <- numeric(numfolds)
startval <- 1
for(kth in (1:numfolds)){
  endval <- vct.sizes[kth] + startval - 1
  
  qda_model <- qda(Value ~ lstat+rm+ptratio,
                   family=binomial, 
                   data = training_testing[-(startval:endval), ])

  qda.pred.vals <- predict(qda_model,
                           newdata = training_testing[startval:endval, ],
                           type="response")
  
  
  
  ## print(tbl.results)
  
  tbl.results <- table(training_testing[startval:endval,]$Value,
                       qda.pred.vals$class)
  
  accuracy <- (round((tbl.results[1,1] + tbl.results[2,2])/sum(
    
    1,tbl.results), 4)*100)
 
  
  accuracies[kth] <- accuracy
  startval <- endval + 1
}

overall.qda.accuracy <- mean(accuracies)
print(paste("For the best qda model, the overall k-fold CV accuracy is:",
            round(overall.qda.accuracy,5)))

```

knn

```{r}
## \KNN.
rm(list = ls())
library(MASS)
library(class)

data(Boston)
housingdataset <- as.data.frame(Boston) # attach a label to the dataframe object

Value <- ifelse(housingdataset$medv > median(housingdataset$medv),
                "High", "Low")
Value <- as.factor(Value)
summary(Value)

housingdataset <- data.frame(housingdataset, Value)
housingdataset$chas <- as.factor(housingdataset$chas)
summary(housingdataset)

for (iternum in (1:3)){
  print(paste("Shuffle number:", iternum))
  housingdataset <- housingdataset[sample(nrow(housingdataset)), ]
}

sz <- nrow(housingdataset) * 0.90
training_testing <- housingdataset[1:sz,]
holdout <- housingdataset[(sz):nrow(housingdataset),]

trn <- cbind(training_testing$lstat,
             training_testing$rm, 
             training_testing$ptratio)

tst <- cbind(holdout$lstat, holdout$rm, holdout$ptratio)

cl <- training_testing$Value

preds <- knn(trn, tst, cl, k = 6)
tbl.results <- table(holdout$Value, preds)

print(tbl.results)
accuracy <- (round((tbl.results[1,1] + tbl.results[2,2])/sum(tbl.results), 4)*100)

overall.knn.accuracy <- mean(accuracy)
print(paste("For the best knn model, the overall k-fold CV accuracy is:",
            round(overall.knn.accuracy, 4)))
```

```{r}
preds <- knn(trn, tst, cl, k = 8)
tbl.results <- table(holdout$Value, preds)

print(tbl.results)
accuracy2 <- (round((tbl.results[1,1] + tbl.results[2,2])/sum(tbl.results), 4)*100)

overall.knn.accuracy2 <- mean(accuracy2)
print(paste("For the best knn model, the overall k-fold CV accuracy is:",
            round(overall.knn.accuracy2,4)))
```

```{r}
preds <- knn(trn, tst, cl, k = 10)
tbl.results <- table(holdout$Value, preds)

print(tbl.results)
accuracy3 <- (round((tbl.results[1,1] + tbl.results[2,2])/sum(tbl.results), 4)*100)

overall.knn.accuracy3 <- mean(accuracy3)
print(paste("For the best knn model, the overall k-fold CV accuracy is:",
            round(overall.knn.accuracy3, 4)))
```

#KNN: 
When K VALUE is taken as 6, 8 and 12, the accuracy of the model with k value = 8 (88.24) is higher when compared with the other two models with accuracies 84.31 and 86.27 respectively.


################################glm holdout#####################################
```{r}
## Using k-fold CV for building and estimating the performance of a glm model
library(MASS)
library(class)
data(Boston)
housingdataset <- as.data.frame(Boston) # attach a label to the dataframe object

Value <- ifelse(housingdataset$medv > median(housingdataset$medv),
                "High", "Low")
Value <- as.factor(Value)
summary(Value)

housingdataset <- data.frame(housingdataset, Value)
housingdataset$chas <- as.factor(housingdataset$chas)
summary(housingdataset)

for (iternum in (1:3)){
  print(paste("Shuffle number:", iternum))
  housingdataset <- housingdataset[sample(nrow(housingdataset)), ]
}

sz <- nrow(housingdataset) * 0.90
training_testing <- housingdataset[1:sz,]
holdout <- housingdataset[(sz):nrow(housingdataset),]

## k-fold CV

samplesize <- nrow(training_testing)
numfolds <- 7

quotient <- samplesize %/% numfolds
remainder <- samplesize %% numfolds 

vct.sizes <- rep(quotient, numfolds) 
if(remainder > 0){
  for(i in 1:remainder){
    vct.sizes[i] <- vct.sizes[i] + 1 
  }
}

print(paste("K:", numfolds, "n:", samplesize))
print(vct.sizes)

set.seed(100)
training_testing <- training_testing[sample(nrow(training_testing)), ]


accuracies <- numeric(numfolds)
startval <- 1
for(kth in (1:numfolds)){
  endval <- vct.sizes[kth] + startval - 1
  
  glm_modelh <- glm(Value ~ lstat+rm+ptratio,
                   family=binomial, 
                   data = holdout)
  hglm.pred.vals <- predict(glm_modelh,
                           newdata = holdout,
                           type="response")
  hglm.predictions <- rep("High", length(hglm.pred.vals))
  hglm.predictions [hglm.pred.vals > 0.50] <- "Low" 
  
  htbl.results <- table(holdout$Value,hglm.predictions)
  

  
  print(htbl.results)
  
  ## print(tbl.results)
  
  hhaccuracy <- (round((htbl.results[1,1] + htbl.results[2,2])/sum(htbl.results), 4)*100)
  
  accuracies[kth] <- hhaccuracy
  startval <- endval + 1
}

overall.hglm.accuracy <- mean(accuracies)
print(paste("For the best glm model, the overall k-fold CV accuracy is:",
            round(overall.hglm.accuracy, 5)))
```

```{r}
## 
rm(list = ls())
library(MASS)
library(class)

data(Boston)
housingdataset <- as.data.frame(Boston) # attach a label to the dataframe object

Value <- ifelse(housingdataset$medv > median(housingdataset$medv),
                "High", "Low")
Value <- as.factor(Value)
summary(Value)

housingdataset <- data.frame(housingdataset, Value)
housingdataset$chas <- as.factor(housingdataset$chas)
summary(housingdataset)

for (iternum in (1:3)){
  print(paste("Shuffle number:", iternum))
  housingdataset <- housingdataset[sample(nrow(housingdataset)), ]
}

sz <- nrow(housingdataset) * 0.90
training_testing <- housingdataset[1:sz,]
holdout <- housingdataset[(sz):nrow(housingdataset),]

trn <- cbind(training_testing$lstat,
             training_testing$rm, 
             training_testing$ptratio)

tst <- cbind(holdout$lstat, holdout$rm, holdout$ptratio)

cl <- training_testing$Value


## The overall k-fold-based CV approach
samplesize <- nrow(housingdataset)
samplesize

numfolds <- 11 # we're setting k = 11

quotient <- samplesize %/% numfolds # the %/% operator returns the quotient of a division
remainder <- samplesize %% numfolds # the %% operator returns the remainder of a division

vct.sizes <- rep(quotient, numfolds) # create a vector representing the initial subsets with size = quotient
if(remainder > 0){
    for(i in 1:remainder){
        vct.sizes[i] <- vct.sizes[i] + 1 # for the "remainder" number of subsets, add one to their size
    }
}

print(paste("K:", 11, "n:", samplesize))


accuracies <- numeric(numfolds)
startval <- 1
for(kth in (1:numfolds)){
  endval <- vct.sizes[kth] + startval - 1
  
  hLDA_model <- lda(Value ~ lstat+rm+ptratio,
                   family=binomial, 
                   data = holdout)

  hlda.pred.vals <- predict(hLDA_model,
                           newdata = holdout,
                           type="response")
  
  
  htbl.results <- table(holdout$Value,
                       hlda.pred.vals$class)
  
  
  print(htbl.results)
  
  accuracyh <- (round((htbl.results[1,1] + htbl.results[2,2])/sum(
    
    1,htbl.results), 4)*100)
  
  accuracies[kth] <- accuracyh
  startval <- endval + 1
}

overall.hlda.accuracy <- mean(accuracies)
print(paste("For the best hlda model, the overall k-fold CV accuracy is:",
            round(overall.hlda.accuracy,5)))
```

```{r}
## the following is the set up needed for KNN.
rm(list = ls())
library(MASS)
library(class)

data(Boston)
housingdataset <- as.data.frame(Boston) # attach a label to the dataframe object

Value <- ifelse(housingdataset$medv > median(housingdataset$medv),
                "High", "Low")
Value <- as.factor(Value)
summary(Value)

housingdataset <- data.frame(housingdataset, Value)
housingdataset$chas <- as.factor(housingdataset$chas)
summary(housingdataset)

for (iternum in (1:3)){
  print(paste("Shuffle number:", iternum))
  housingdataset <- housingdataset[sample(nrow(housingdataset)), ]
}

sz <- nrow(housingdataset) * 0.90
training_testing <- housingdataset[1:sz,]
holdout <- housingdataset[(sz):nrow(housingdataset),]

trn <- cbind(training_testing$lstat,
             training_testing$rm, 
             training_testing$ptratio)

tst <- cbind(holdout$lstat, holdout$rm, holdout$ptratio)

cl <- training_testing$Value


## The overall k-fold-based CV approach
samplesize <- nrow(housingdataset)
samplesize

numfolds <- 11 # we're setting k = 11

quotient <- samplesize %/% numfolds # the %/% operator returns the quotient of a division
remainder <- samplesize %% numfolds # the %% operator returns the remainder of a division

vct.sizes <- rep(quotient, numfolds) # create a vector representing the initial subsets with size = quotient
if(remainder > 0){
    for(i in 1:remainder){
        vct.sizes[i] <- vct.sizes[i] + 1 # for the "remainder" number of subsets, add one to their size
    }
}

print(paste("K:", 11, "n:", samplesize))


accuracies <- numeric(numfolds)
startval <- 1
for(kth in (1:numfolds)){
  endval <- vct.sizes[kth] + startval - 1
  
  hQDA_model <- qda(Value ~ lstat+rm+ptratio,
                   family=binomial, 
                   data = holdout)

  hQda.pred.vals <- predict(hQDA_model,
                           newdata = holdout,
                           type="response")
  
  
  htbl.resultsS <- table(holdout$Value,
                       hQda.pred.vals$class)
  
  
  print(htbl.resultsS)
  
  accuracyhS <- (round((htbl.resultsS[1,1] + htbl.resultsS[2,2])/sum(
    
    1,htbl.resultsS), 4)*100)
  
  accuracies[kth] <- accuracyhS
  startval <- endval + 1
}

overall.hQda.accuracy <- mean(accuracies)
print(paste("For the best hQda model, the overall k-fold CV accuracy is:",
            round(overall.hQda.accuracy,5)))
```

```{r}
## the following is the set up needed for KNN.
rm(list = ls())
library(MASS)
library(class)

data(Boston)
housingdataset <- as.data.frame(Boston) # attach a label to the dataframe object

Value <- ifelse(housingdataset$medv > median(housingdataset$medv),
                "High", "Low")
Value <- as.factor(Value)
summary(Value)

housingdataset <- data.frame(housingdataset, Value)
housingdataset$chas <- as.factor(housingdataset$chas)
summary(housingdataset)

for (iternum in (1:3)){
  print(paste("Shuffle number:", iternum))
  housingdataset <- housingdataset[sample(nrow(housingdataset)), ]
}

sz <- nrow(housingdataset) * 0.90
training_testing <- housingdataset[1:sz,]
holdout <- housingdataset[(sz):nrow(housingdataset),]

## k-fold CV

samplesize <- nrow(training_testing)
numfolds <- 5

quotient <- samplesize %/% numfolds
remainder <- samplesize %% numfolds 

vct.sizes <- rep(quotient, numfolds) 
if(remainder > 0){
  for(i in 1:remainder){
    vct.sizes[i] <- vct.sizes[i] + 1 
  }
}

print(paste("K:", numfolds, "n:", samplesize))
print(vct.sizes)

set.seed(99)
training_testing <- training_testing[sample(nrow(training_testing)), ]

## Using k-fold CV for building and estimating the performance of a logistic HOLDOUT
## regression model
accuracies <- numeric(numfolds)
startval <- 1
for(kth in (1:numfolds)){
  endval <- vct.sizes[kth] + startval - 1
  

trn <- cbind(training_testing$lstat,
             training_testing$rm, 
             training_testing$ptratio)

tst <- cbind(holdout$lstat, holdout$rm, holdout$ptratio)

cl <- training_testing$Value

preds <- knn(trn, tst, cl, k = 3)


tbl.results <- table(holdout$Value, preds)

print(tbl.results)

accuracy <- (round((tbl.results[1,1] + tbl.results[2,2])/sum(tbl.results), 4)*100)

accuracies[kth] <- accuracy
  startval <- endval + 1
}

overall.knn.accuracy <- mean(accuracy)
print(paste("For the best knn model, the overall k-fold CV accuracy is:",
            round(overall.knn.accuracy, 4)))
```
################################################################
```{R}
#TRAINING KNN
rm(list = ls())
library(MASS)
library(class)

data(Boston)
housingdataset <- as.data.frame(Boston) # attach a label to the dataframe object

Value <- ifelse(housingdataset$medv > median(housingdataset$medv),
                "High", "Low")
Value <- as.factor(Value)
summary(Value)

housingdataset <- data.frame(housingdataset, Value)
housingdataset$chas <- as.factor(housingdataset$chas)
summary(housingdataset)

for (iternum in (1:3)){
  print(paste("Shuffle number:", iternum))
  housingdataset <- housingdataset[sample(nrow(housingdataset)), ]
}

sz <- nrow(housingdataset) * 0.90
training_testing <- housingdataset[1:sz,]
holdout <- housingdataset[(sz):nrow(housingdataset),]

## k-fold CV

samplesize <- nrow(training_testing)
numfolds <- 5

quotient <- samplesize %/% numfolds
remainder <- samplesize %% numfolds 

vct.sizes <- rep(quotient, numfolds) 
if(remainder > 0){
  for(i in 1:remainder){
    vct.sizes[i] <- vct.sizes[i] + 1 
  }
}

print(paste("K:", numfolds, "n:", samplesize))
print(vct.sizes)

set.seed(99)
training_testing <- training_testing[sample(nrow(training_testing)), ]

## Using k-fold CV for building and estimating the performance of a logistic
## regression model
accuracies <- numeric(numfolds)
startval <- 1
for(kth in (1:numfolds)){
  endval <- vct.sizes[kth] + startval - 1
trn <- cbind(housingdataset[-(startval:endval), ]$lstat,
            housingdataset[-(startval:endval), ]$rm,
            housingdataset[-(startval:endval), ]$ptratio)

  tst <- cbind(housingdataset[startval:endval, ]$lstat,
               housingdataset[startval:endval, ]$rm,
               housingdataset[startval:endval, ]$ptratio)

  cl <- housingdataset[-(startval:endval), ]$Value

  preds <- knn(trn, tst, cl, k = 3)
  tbl.results <- table(housingdataset[startval:endval, ]$Value, preds)

  #print(tbl.results)

  accuracy <- (round((tbl.results[1,1] + tbl.results[2,2])/sum(tbl.results), 4)*100)

  accuracies[kth] <- accuracy
  startval <- endval + 1
}
print(paste0("Avg. CV accuracy: ",
             mean(accuracies)))
```
```{r}
accuracies <- numeric(numfolds)
startval <- 1
for(kth in (1:numfolds)){
  endval <- vct.sizes[kth] + startval - 1
trn <- cbind(housingdataset[-(startval:endval), ]$lstat,
            housingdataset[-(startval:endval), ]$rm,
            housingdataset[-(startval:endval), ]$ptratio)

  tst <- cbind(housingdataset[startval:endval, ]$lstat,
               housingdataset[startval:endval, ]$rm,
               housingdataset[startval:endval, ]$ptratio)

  cl <- housingdataset[-(startval:endval), ]$Value

  preds <- knn(trn, tst, cl, k = 6)
  tbl.results <- table(housingdataset[startval:endval, ]$Value, preds)

  #print(tbl.results)

  accuracy <- (round((tbl.results[1,1] + tbl.results[2,2])/sum(tbl.results), 4)*100)

  accuracies[kth] <- accuracy
  startval <- endval + 1
}
print(paste0("Avg. CV accuracy: ",
             mean(accuracies)))
```

```{r}
accuracies <- numeric(numfolds)
startval <- 1
for(kth in (1:numfolds)){
  endval <- vct.sizes[kth] + startval - 1
trn <- cbind(housingdataset[-(startval:endval), ]$lstat,
            housingdataset[-(startval:endval), ]$rm,
            housingdataset[-(startval:endval), ]$ptratio)

  tst <- cbind(housingdataset[startval:endval, ]$lstat,
               housingdataset[startval:endval, ]$rm,
               housingdataset[startval:endval, ]$ptratio)

  cl <- housingdataset[-(startval:endval), ]$Value

  preds <- knn(trn, tst, cl, k = 8)
  tbl.results <- table(housingdataset[startval:endval, ]$Value, preds)

  #print(tbl.results)

  accuracy <- (round((tbl.results[1,1] + tbl.results[2,2])/sum(tbl.results), 4)*100)

  accuracies[kth] <- accuracy
  startval <- endval + 1
}
print(paste0("Avg. CV accuracy: ",
             mean(accuracies)))
```

```{r}
accuracies <- numeric(numfolds)
startval <- 1
for(kth in (1:numfolds)){
  endval <- vct.sizes[kth] + startval - 1
trn <- cbind(housingdataset[-(startval:endval), ]$lstat,
            housingdataset[-(startval:endval), ]$rm,
            housingdataset[-(startval:endval), ]$ptratio)

  tst <- cbind(housingdataset[startval:endval, ]$lstat,
               housingdataset[startval:endval, ]$rm,
               housingdataset[startval:endval, ]$ptratio)

  cl <- housingdataset[-(startval:endval), ]$Value

  preds <- knn(trn, tst, cl, k = 12)
  tbl.results <- table(housingdataset[startval:endval, ]$Value, preds)

  #print(tbl.results)

  accuracy <- (round((tbl.results[1,1] + tbl.results[2,2])/sum(tbl.results), 4)*100)

  accuracies[kth] <- accuracy
  startval <- endval + 1
}
print(paste0("Avg. CV accuracy: ",
             mean(accuracies)))
```